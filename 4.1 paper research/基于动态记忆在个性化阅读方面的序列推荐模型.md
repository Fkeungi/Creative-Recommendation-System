# 基于动态记忆在个性化阅读方面的序列推荐模型

## 1.拟写的论文标题

(1) 中文标题：基于动态记忆在个性化阅读方面的序列推荐模型

(2) 英文标题：A sequential recommendation model for personalized reading based on dynamic memory

(3) 中文关键词：动态记忆、记忆网络、序列推荐、注意力机制

(4) 英文关键词：dynamic memory、Memory Networks、Sequential Recommendation、Memory Networks

## 2.现实意义

传统的序列推荐方法主要采用固定的记忆结构来表示用户的历史行为，这种方法忽略了用户兴趣的演化过程，无法准确地反映用户当前的兴趣。相比之下，基于动态记忆的序列推荐方法可以通过不断更新记忆结构来反映用户的兴趣演化过程，从而提高推荐的准确性。

## 3.创新点介绍

基于动态记忆的序列推荐方法通常包括两个主要部分，即记忆更新和推荐。在记忆更新部分，该方法会对用户的历史行为序列进行编码，并将编码结果加入动态记忆中。而在推荐部分，该方法则会利用动态记忆结构和当前的用户行为序列，来预测用户可能感兴趣的下一个行为。

个性化阅读

## 4.文献综述

序列推荐是推荐系统领域中的一个重要研究方向，其目标是根据用户的历史行为序列，预测用户的下一个行为，并向其推荐相关内容。近年来，序列推荐领域的研究和算法发展迅速，其中有一些具有代表性的研究和算法。

传统的序列模型主要包括基于马尔可夫模型、隐马尔可夫模型和马尔可夫链蒙特卡罗方法的序列预测模型。其中，马尔可夫模型是最简单的序列模型之一，它基于当前状态的概率分布，预测下一个状态的分布。隐马尔可夫模型则是一种更加复杂的序列模型，它考虑了序列中的状态转移和观测值之间的关系。马尔可夫链蒙特卡罗方法则是一种基于蒙特卡罗采样的序列预测算法，可以对序列中的长期依赖进行建模。传统的序列模型在序列预测中有一定的应用，但在处理长序列和复杂数据时存在一定的局限性。

而深度学习技术在序列推荐中的应用也取得了很大的进展，基于深度学习的序列模型主要包括循环神经网络 (RNN)、长短时记忆网络 (LSTM)、门控循环单元网络 (GRU) 等。这些模型在处理长序列和复杂数据方面具有很好的表现，并且能够有效地处理序列中的长期依赖关系。在序列推荐中，这些模型可以对用户的历史行为序列进行建模，从而预测用户的下一个行为并进行推荐。Gru4Rec模型使用GRU网络来捕捉用户的行为序列，并使用候选物品的向量来更新用户的状态

另外注意力机制在序列推荐中的应用也取得了很大的进展。基于注意力机制的序列模型可以更加准确地捕捉用户的兴趣演化和行为模式，从而提高推荐的准确性和个性化程度。其中，自注意力机制和多头注意力机制是最常见的注意力机制，它们可以根据不同的上下文信息，对序列中的不同部分进行加权和聚合。

基于记忆网络或自适应记忆的动态记忆模型通常使用记忆网络来对用户历史行为进行建模，并使用 ***注意力机制*** 和其他技术来处理长期依赖和上下文信息。MemN2N模型使用多个记忆块来存储不同的历史行为，并使用自注意力机制来计算权重，从而预测下一个行为。SR-GNN模型使用自适应记忆网络来对用户历史行为进行建模，并使用注意力机制来计算每个历史行为的重要性，从而预测下一个行为。

## 5.该方向已有研究

### 5.1 顺序通过时间感知注意力记忆网络推荐

(1) 论文标题：Sequential Recommender via Time-aware Attentive Memory Network

(2) 论文链接：<https://dro.dur.ac.uk/31779/1/31779.pdf>

(3) 开源代码：<https://github.com/cocoandpudding/MTAMRecommender>

(4) 数据集：

<https://grouplens.org/datasets/movielens/20m/>
<http://deepyeti.ucsd.edu/jianmo/amazon/index.html>
<http://2015.recsyschallenge.com/challenge.html>
<https://tianchi.aliyun.com/dataset/dataDetail?dataId=46>

(5) 论文摘要：推荐系统旨在帮助用户从不断增长的项目语料库中发现最喜欢的内容。尽管通过深度学习，推荐器已经有了很大的改进，但它们仍然面临着一些挑战：i）行为比句子中的单词复杂得多，因此传统的注意力和循环模型在捕捉用户偏好的时间动态方面存在局限性。ii）用户的偏好是多重的，并且是不断发展的，因此很难将长期记忆和短期意图结合起来。

在本文中，我们提出了一种时间门控方法来改进注意力机制和循环单元，以便在信息过滤和状态转换中同时考虑时间信息。此外，我们提出了一种混合顺序推荐器，称为多跳时间感知注意力记忆网络（MTAM），以整合长期和短期偏好。我们使用提出的时间感知GRU网络来学习短期意图并在用户内存中维护先前的记录。我们将短期意图视为查询，并通过提出的时间感知注意力设计多跳内存读取操作，以基于当前意图和长期记忆生成用户表示。我们的方法对于候选检索任务是可扩展的，并且可以被视为基于点积的 Top-K 推荐的潜在分解的非线性泛化。最后，我们在六个基准数据集上进行了广泛的实验，实验结果证明了我们的MTAM和时间门控方法的有效性。

### 5.2 带动态内存网络的门控顺序推荐

(1) 论文标题：Gated Sequential Recommendation with Dynamic Memory Network

(2) 论文链接：<https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8852311>

(3) 数据集：

<http://jmcauley.ucsd.edu/data/amazon/>
<https://tianchi.shuju.aliyun.com/datalab/dataSet.htm?id=1>

(4) 论文摘要：推荐系统根据用户的历史行为为用户提供项目排名列表。现有的方法通常将用户嵌入他们的时间动态，这可能已经失去了他们长期的内在偏好。RNN因其对顺序信息建模的表现力而被广泛用于对用户行为进行建模。然而，由于短期依赖性问题，基于RNN的方法不足以捕获用户的内在偏好。本文旨在从两个角度取得进展：1.更有效、更稳健地从用户的历史行为中提取用户的内在偏好;2. 探索一种微调方式，将用户的内在偏好与外部动态兴趣相结合。为此，我们建议使用门控机制修改动态记忆网络，为单个用户生成动态，精确的表示。我们还设计了一种自动和隐式方法，它利用内在偏好作为触发信号来搜索输入序列，并在内存模块中检索用户的时间动态。结果，实现了时间动态和内在偏好的理想组合。在三个基准数据集上进行的广泛实验表明，我们的模型优于现有的基线方法。对各跳的注意力权重的进一步分析证明了所提方法的有效性。

### 5.3 基于动态记忆的顺序推荐注意力网络

(1) 论文标题：Dynamic Memory based Attention Network for Sequential Recommendation

(2) 论文链接：<https://arxiv.org/pdf/2102.09269.pdf>

(3) 数据集：<https://grouplens.org/datasets/movielens/1m/>

(4) 论文摘要：顺序推荐在各种在线服务中变得越来越重要。它旨在从用户的历史交互中对用户的动态偏好进行建模，并预测他们的下一个项目。真实系统上累积的用户行为记录可能很长。这些丰富的数据为跟踪用户的实际兴趣带来了机会。之前的工作主要集中在根据相对较新的行为提出建议。但是，整体顺序数据可能无法有效利用，因为早期交互可能会影响用户当前的选择。此外，在为每个用户执行推理时扫描整个行为序列变得无法忍受，因为现实世界的系统需要较短的响应时间。为了弥合这一差距，我们提出了一种新的长顺序推荐模型，称为基于动态记忆的注意力网络（DMAN）。它将整个长行为序列分割成一系列子序列，然后训练模型并维护一组内存块，以保持用户的长期利益。为了提高内存保真度，DMAN 通过最小化辅助重建损失，将每个用户的长期兴趣动态抽象到自己的内存块中。基于动态记忆，可以明确提取用户的短期和长期兴趣，进行组合，实现高效的联合推荐。四个基准数据集的实证结果表明，我们的模型在捕获长期依赖性方面优于各种最先进的顺序模型。

## 6.使用的推荐算法

Gru4Rec、Session-based RNN（循环神经网络）

MemN2N、DREAM（记忆网络）

SR-GNN、GRU4Rec+（自适应记忆）

## 7.实验数据集

<https://www.yelp.com/dataset>

<https://tianchi.aliyun.com/dataset/dataDetail?dataId=46>

## 8.可行性评估

从多样性和覆盖率对结果进行评估是我们可以做到的（推荐的物品之间的差异性和推荐到不同物品的数量）。

通过对比实验比较一两种其他普通序列推荐算法或者模型，从而说明结果的准确、优越性。

## 9.参考文献

<https://github.com/RUCAIBox/Awesome-RSPapers#attack-in-recommender-system>

<https://www.zhihu.com/column/c_1330637706267734016>

<https://zhuanlan.zhihu.com/p/585945102>
